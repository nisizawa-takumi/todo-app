# LLM機能実験中に質問したこと（QA形式）

---

## Q. Dockerfileでパッケージをインストールする場合と、docker-compose.ymlのcommandでインストールする場合の違いは？

**A.**
両者の違いは「どのタイミングで」「どの層（イメージ or コンテナ）」にパッケージがインストールされるか、です。

- **Dockerfileでイメージビルド時にインストール**
  - タイミング: `docker build` 時（イメージ作成時）
  - インストール先: 作成されるDockerイメージの中
  - メリット:
    - どのコンテナでも、イメージから起動すれば最初からパッケージが入っている
    - 再起動やexecで入っても常に使える
    - ビルドキャッシュが効くので、起動が速い
  - デメリット:
    - 依存パッケージを変えたら再ビルドが必要

- **docker-compose.ymlのcommandでインストール**
  - タイミング: コンテナ起動時（`docker compose up` 時）に毎回コマンドでインストール
  - インストール先: 起動したそのコンテナの一時的なファイルシステム
  - メリット:
    - 依存パッケージをすぐに変えられる（ビルド不要）
  - デメリット:
    - 毎回インストールが走るので起動が遅い
    - execで新しいシェルを開くとパッケージが使えない場合がある
    - 再起動時にパッケージが消えることもある

**結論:**  
本番や開発で安定して使いたい場合はDockerfileでイメージビルド時にインストールが推奨です。  
一時的な検証や素早い試行錯誤ならcommandでのインストールも可ですが、安定性・再現性は低いです。

---

## Q. Ollama（オラマ）とは何ですか？

**A.**
Ollamaは、ローカル環境で大規模言語モデル（LLM）を簡単に動かすためのオープンソースのツール／サーバーです。

### 主な特徴
- **ローカルで動作**  
  PCやサーバー上で、ChatGPTのようなLLMを自分のマシンで直接動かせます（クラウド不要）。
- **多様なモデル対応**  
  Llama、Gemma、Qwen、ELYZAなど、さまざまなオープンソースLLMをサポート。
- **APIサーバー機能**  
  OpenAI API互換のエンドポイントを提供し、curlやPythonのrequests、各種アプリから簡単に利用可能。
- **簡単なモデル管理**  
  `ollama pull モデル名`でモデルのダウンロード、`ollama run モデル名`で対話開始など、コマンドがシンプル。
- **CPU/GPU両対応**  
  GPUがあれば高速、CPUだけでも動作可能（ただし速度は遅くなります）。

---

## Q. Ollamaの使い方は？

**A.**

### モデルのダウンロード
```
ollama pull llama2
```

### 対話開始
```
ollama run llama2
```

### APIサーバーとして利用
サーバーを立てて、他のアプリやスクリプトからHTTPリクエストでLLMを利用できます。

---

## Q. Ollamaはどんなときに便利ですか？

**A.**
Ollamaは「ローカルでLLMを簡単に動かすためのサーバー＆CLIツール」です。  
開発・検証・個人利用などで、クラウドに頼らずLLMを使いたい場合に非常に便利です。
